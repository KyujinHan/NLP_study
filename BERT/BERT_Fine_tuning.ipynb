{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ae21a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 파인 튜닝 해보기\n",
    "# 파이썬-딥러닝-파이토치 책의 일부를 가져옴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f342c72",
   "metadata": {},
   "source": [
    "- Fine tuning 이란?  \n",
    "기존에 학습되어져 있는 모델을 기반으로 아키텍쳐를 새로운 목적(나의 이미지 데이터에 맞게)변형하고,  \n",
    "이미 학습된 모델 Weights로 부터 학습을 업데이트하는 방법을 말한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb32e95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 1. Import Module '''\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4962b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'dog', 'is', 'cute', '.', 'he', 'likes', 'playing', '.', 'i', 'bought', 'a', 'pet', 'food', 'for', 'him']\n"
     ]
    }
   ],
   "source": [
    "''' 2. Tokenizer 만들기 '''\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# test\n",
    "sentence = \"My dog is cute. He likes playing. I bought a  pet food for him\"\n",
    "print(tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "425c5de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab) # vocab의 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f7312e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "max_seq_length # 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fedb399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_tokenizer(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    tokens = tokens[:max_seq_length-2] # [CLS], [SEP] token 고려\n",
    "    return tokens\n",
    "\n",
    "# Text 전처리\n",
    "def PreProcessingText(input_sentence):\n",
    "    input_sentence = input_sentence.lower() # 소문자화\n",
    "    input_sentence = re.sub('<[^>]*>', repl= ' ', string = input_sentence) # \"<br />\" 처리\n",
    "    input_sentence = re.sub('[!\"$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~]', repl= ' ', string = input_sentence) # 특수문자 처리 (\"'\" 제외)\n",
    "    input_sentence = re.sub('\\s+', repl= ' ', string = input_sentence) # 연속된 띄어쓰기 처리\n",
    "    if input_sentence:\n",
    "        return input_sentence\n",
    "\n",
    "# Token 설정(전처리)하기\n",
    "def PreProc(list_sentence):\n",
    "    return [tokenizer.convert_tokens_to_ids(PreProcessingText(x)) for x in list_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "831c6fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 3. Field 만들기 '''\n",
    "TEXT = data.Field(batch_first = True,\n",
    "                 use_vocab = False,\n",
    "                 tokenize = new_tokenizer,\n",
    "                 preprocessing = PreProc,\n",
    "                 init_token = tokenizer.cls_token_id,\n",
    "                 eos_token = tokenizer.eos_token_id,\n",
    "                 pad_token = tokenizer.pad_token_id,\n",
    "                 unk_token = tokenizer.unk_token_id)\n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f59231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "258d1a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1d5447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = train_data.split(random_state = random.seed(0), split_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e146c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ea4e434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b45a381e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': <torchtext.data.field.Field at 0x17e0f2731f0>,\n",
       " 'label': <torchtext.data.field.LabelField at 0x17e0f2730d0>}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92c83a1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['another',\n",
       " 'in',\n",
       " 'a',\n",
       " 'long',\n",
       " 'line',\n",
       " 'of',\n",
       " 'flick',\n",
       " '##s',\n",
       " 'made',\n",
       " 'by',\n",
       " 'people',\n",
       " 'who',\n",
       " 'think',\n",
       " 'that',\n",
       " 'knowing',\n",
       " 'how',\n",
       " 'to',\n",
       " 'operate',\n",
       " 'a',\n",
       " 'camera',\n",
       " 'is',\n",
       " 'the',\n",
       " 'same',\n",
       " 'as',\n",
       " 'telling',\n",
       " 'a',\n",
       " 'story',\n",
       " '[UNK]',\n",
       " 'within',\n",
       " '15',\n",
       " 'minutes',\n",
       " '[UNK]',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'premise',\n",
       " 'is',\n",
       " 'laid',\n",
       " 'out',\n",
       " 'in',\n",
       " 'just',\n",
       " 'a',\n",
       " 'few',\n",
       " 'lines',\n",
       " '[UNK]',\n",
       " 'so',\n",
       " 'there',\n",
       " 'is',\n",
       " 'absolutely',\n",
       " 'no',\n",
       " 'mystery',\n",
       " '[UNK]',\n",
       " 'which',\n",
       " 'eliminate',\n",
       " '##s',\n",
       " 'a',\n",
       " 'whole',\n",
       " 'face',\n",
       " '##t',\n",
       " 'of',\n",
       " 'the',\n",
       " 'suspense',\n",
       " '[UNK]',\n",
       " 'the',\n",
       " 'only',\n",
       " 'half',\n",
       " '[UNK]',\n",
       " 'way',\n",
       " 'competent',\n",
       " 'actor',\n",
       " 'is',\n",
       " 'killed',\n",
       " '10',\n",
       " 'minutes',\n",
       " 'into',\n",
       " 'the',\n",
       " 'film',\n",
       " '[UNK]',\n",
       " 'so',\n",
       " 'we',\n",
       " \"'\",\n",
       " 're',\n",
       " 'left',\n",
       " 'with',\n",
       " 'stupid',\n",
       " 'characters',\n",
       " 'running',\n",
       " 'around',\n",
       " 'doing',\n",
       " 'stupid',\n",
       " 'things',\n",
       " '[UNK]',\n",
       " 'low',\n",
       " 'budget',\n",
       " 'films',\n",
       " 'can',\n",
       " \"'\",\n",
       " 't',\n",
       " 'afford',\n",
       " 'expensive',\n",
       " 'special',\n",
       " 'effects',\n",
       " '[UNK]',\n",
       " 'so',\n",
       " 'the',\n",
       " 'c',\n",
       " '##gi',\n",
       " 'portions',\n",
       " 'are',\n",
       " 'un',\n",
       " '##sur',\n",
       " '##pr',\n",
       " '##ising',\n",
       " '##ly',\n",
       " 'un',\n",
       " '##im',\n",
       " '##pressive',\n",
       " '[UNK]',\n",
       " 'but',\n",
       " 'were',\n",
       " 'at',\n",
       " 'least',\n",
       " 'a',\n",
       " 'valid',\n",
       " 'attempt',\n",
       " '[UNK]',\n",
       " 'the',\n",
       " 'creature',\n",
       " 'suit',\n",
       " 'is',\n",
       " 'terrible',\n",
       " '[UNK]',\n",
       " 'as',\n",
       " 'seen',\n",
       " 'when',\n",
       " 'it',\n",
       " 'falls',\n",
       " 'to',\n",
       " 'the',\n",
       " 'sidewalk',\n",
       " '[UNK]',\n",
       " 'and',\n",
       " 'the',\n",
       " 'director',\n",
       " 'keeps',\n",
       " 'emphasizing',\n",
       " 'the',\n",
       " 'eyes',\n",
       " '[UNK]',\n",
       " 'which',\n",
       " 'aren',\n",
       " \"'\",\n",
       " 't',\n",
       " 'even',\n",
       " 'the',\n",
       " 'red',\n",
       " 'color',\n",
       " 'shown',\n",
       " 'in',\n",
       " 'mirror',\n",
       " 'shots',\n",
       " '[UNK]',\n",
       " 'the',\n",
       " 'dialogue',\n",
       " 'is',\n",
       " 'clumsy',\n",
       " 'and',\n",
       " 'un',\n",
       " '##ins',\n",
       " '##pired',\n",
       " '[UNK]',\n",
       " 'with',\n",
       " 'some',\n",
       " 'lines',\n",
       " 'reminiscent',\n",
       " 'of',\n",
       " 'aliens',\n",
       " 'or',\n",
       " 'term',\n",
       " '##inator',\n",
       " '[UNK]',\n",
       " 'the',\n",
       " 'last',\n",
       " 'action',\n",
       " 'sequence',\n",
       " 'takes',\n",
       " 'place',\n",
       " 'in',\n",
       " 'a',\n",
       " 'police',\n",
       " 'station',\n",
       " '[UNK]',\n",
       " 'also',\n",
       " 'a',\n",
       " 'rip',\n",
       " '[UNK]',\n",
       " 'off',\n",
       " 'from',\n",
       " 'term',\n",
       " '##inator',\n",
       " '[UNK]',\n",
       " 'with',\n",
       " 'everyone',\n",
       " 'hiding',\n",
       " 'in',\n",
       " 'the',\n",
       " 'one',\n",
       " 'glass',\n",
       " 'lined',\n",
       " 'office',\n",
       " 'that',\n",
       " 'the',\n",
       " 'dark',\n",
       " '##wo',\n",
       " '##lf',\n",
       " 'doesn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'smash',\n",
       " 'into',\n",
       " '[UNK]',\n",
       " 'in',\n",
       " 'the',\n",
       " 'end',\n",
       " '[UNK]',\n",
       " 'the',\n",
       " 'girl',\n",
       " 'calls',\n",
       " 'the',\n",
       " 'hero',\n",
       " '[UNK]',\n",
       " 'a',\n",
       " 'good',\n",
       " 'protector',\n",
       " '[UNK]',\n",
       " '[UNK]',\n",
       " 'but',\n",
       " 'he',\n",
       " 'gets',\n",
       " 'both',\n",
       " 'his',\n",
       " 'partners',\n",
       " '[UNK]',\n",
       " 'the',\n",
       " 'original',\n",
       " 'protector',\n",
       " '[UNK]',\n",
       " 'and',\n",
       " 'at',\n",
       " 'least',\n",
       " 'three',\n",
       " 'other',\n",
       " 'civilians',\n",
       " '[UNK]',\n",
       " 'not',\n",
       " 'to',\n",
       " 'mention',\n",
       " 'a',\n",
       " 'dozen',\n",
       " 'cops',\n",
       " '[UNK]',\n",
       " 'all',\n",
       " 'killed',\n",
       " 'without',\n",
       " 'getting',\n",
       " 'a',\n",
       " 'decent',\n",
       " 'shot',\n",
       " 'off',\n",
       " '[UNK]',\n",
       " 'in',\n",
       " 'spite',\n",
       " 'of',\n",
       " 'an',\n",
       " 'arsenal',\n",
       " 'of',\n",
       " 'silver',\n",
       " 'bullets',\n",
       " 'and',\n",
       " 'a',\n",
       " 'sub',\n",
       " '##mac',\n",
       " '##hine',\n",
       " 'gun',\n",
       " '[UNK]',\n",
       " 'but',\n",
       " 'here',\n",
       " \"'\",\n",
       " 's',\n",
       " 'the',\n",
       " 'real',\n",
       " 'clinch',\n",
       " '##er',\n",
       " 'for',\n",
       " 'bad',\n",
       " 'writing',\n",
       " '[UNK]',\n",
       " 'they',\n",
       " 'could',\n",
       " 'have',\n",
       " 'killed',\n",
       " 'the',\n",
       " 'beast',\n",
       " 'right',\n",
       " 'after',\n",
       " 'the',\n",
       " 'beginning',\n",
       " 'credits',\n",
       " 'when',\n",
       " 'it',\n",
       " 'was',\n",
       " 'holding',\n",
       " 'the',\n",
       " 'strip',\n",
       " '##per',\n",
       " 'while',\n",
       " 'flashing',\n",
       " 'its',\n",
       " 'red',\n",
       " 'eyes',\n",
       " '[UNK]',\n",
       " 'instead',\n",
       " '[UNK]',\n",
       " 'they',\n",
       " 'took',\n",
       " 'it',\n",
       " 'into',\n",
       " 'custody',\n",
       " '[UNK]',\n",
       " '[UNK]',\n",
       " '[UNK]']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(vars(train_data.examples[2])['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d1ba65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lable Examples : \n",
      "\t neg 0\n",
      "\t pos 1\n"
     ]
    }
   ],
   "source": [
    "print('Lable Examples : ')\n",
    "for idx, (k, v) in enumerate(LABEL.vocab.stoi.items()):\n",
    "    print('\\t', k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3bd031a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 4. Make variable '''\n",
    "model_config = {}\n",
    "model_config['batch_size'] = 4\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train_data, valid_data, test_data),\n",
    "                                                                          batch_size = model_config['batch_size'],\n",
    "                                                                          device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5e29b123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 4x371 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 4 (GPU 0)]\n",
      "tensor([[  101,  7929,   100,  ...,   100,  3185,   100],\n",
      "        [  101,  2028,  2305,  ...,     0,     0,     0],\n",
      "        [  101,  2053, 11967,  ...,     0,     0,     0],\n",
      "        [  101,   100, 27594,  ...,     0,     0,     0]], device='cuda:0')\n",
      "tensor([0., 0., 0., 1.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Check batch data\n",
    "sample_for_check = next(iter(train_iterator))\n",
    "print(sample_for_check)\n",
    "print(sample_for_check.text)\n",
    "print(sample_for_check.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c1da546f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "''' 5. Making Model '''\n",
    "BERT = BertModel.from_pretrained('bert-base-uncased')\n",
    "model_config['emb_dim'] = BERT.config.to_dict()['hidden_size'] # 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1db42fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning 작업\n",
    "class SentenceClassification(nn.Module):\n",
    "    def __init__(self, **model_config):\n",
    "        super(SentenceClassification, self).__init__()\n",
    "        \n",
    "        self.bert = BERT\n",
    "        self.fc = nn.Linear(model_config['emb_dim'],\n",
    "                           model_config['output_dim'])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pooled_cls_output = self.bert(x)[1] # [CLS] 토큰에 대한 부분\n",
    "        return F.sigmoid(self.fc(pooled_cls_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4ead9eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 6. Setting '''\n",
    "model_config.update(dict(output_dim=1))\n",
    "model = SentenceClassification(**model_config)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "17f80018",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 7. Train and evaluate function '''\n",
    "def train(model, iterator, optimizer, loss_fn, idx_epoch, **model_config):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    batch_size = model_config['batch_size']\n",
    "    \n",
    "    for idx, batch in enumerate(iterator):\n",
    "        # optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward\n",
    "        prediction = model(batch.text).squeeze()\n",
    "        \n",
    "        loss = loss_fn(prediction, batch.label)\n",
    "        acc = Accuracy(prediction, batch.label)\n",
    "        \n",
    "        # backward\n",
    "        loss.backward() # backpropagation\n",
    "        optimizer.step() # weight update\n",
    "        \n",
    "        if idx % 200 == 0:\n",
    "            print(\"Train Epoch: {}[{}/{}] \\t Train loss: {:.4f} \\t Train acc: {:.4f}\".format(\n",
    "                                    idx_epoch, idx*batch_size, len(iterator)*batch_size, loss.item(), acc.item()))\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss/len(iterator), epoch_acc/len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, optimizer, loss_fn, idx_epoch, **model_config):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    batch_size = model_config['batch_size']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(iterator):\n",
    "            prediction = model(batch.text).squeeze()\n",
    "            loss = loss_fn(prediction, batch.label)\n",
    "            acc = Accuracy(prediction, batch.label)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "    return epoch_loss/len(iterator), epoch_acc/len(iterator)\n",
    "\n",
    "def Accuracy(pred, y):\n",
    "    predict = torch.round(pred)\n",
    "    acc = (predict==y).sum()/len(y)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5d6af125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 4, 'emb_dim': 768, 'output_dim': 1}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "65acc9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Model name : BERT\n",
      "---------------------------------\n",
      "Train Epoch: 1[0/20000] \t Train loss: 0.6928 \t Train acc: 0.2500\n",
      "Train Epoch: 1[800/20000] \t Train loss: 0.6927 \t Train acc: 0.2500\n",
      "Train Epoch: 1[1600/20000] \t Train loss: 0.6932 \t Train acc: 0.5000\n",
      "Train Epoch: 1[2400/20000] \t Train loss: 0.6936 \t Train acc: 0.7500\n",
      "Train Epoch: 1[3200/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 1[4000/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 1[4800/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 1[5600/20000] \t Train loss: 0.6930 \t Train acc: 0.2500\n",
      "Train Epoch: 1[6400/20000] \t Train loss: 0.6929 \t Train acc: 0.2500\n",
      "Train Epoch: 1[7200/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 1[8000/20000] \t Train loss: 0.6930 \t Train acc: 0.2500\n",
      "Train Epoch: 1[8800/20000] \t Train loss: 0.6932 \t Train acc: 0.5000\n",
      "Train Epoch: 1[9600/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 1[10400/20000] \t Train loss: 0.6937 \t Train acc: 0.7500\n",
      "Train Epoch: 1[11200/20000] \t Train loss: 0.6932 \t Train acc: 0.5000\n",
      "Train Epoch: 1[12000/20000] \t Train loss: 0.6955 \t Train acc: 1.0000\n",
      "Train Epoch: 1[12800/20000] \t Train loss: 0.6935 \t Train acc: 0.7500\n",
      "Train Epoch: 1[13600/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 1[14400/20000] \t Train loss: 0.6905 \t Train acc: 0.0000\n",
      "Train Epoch: 1[15200/20000] \t Train loss: 0.6918 \t Train acc: 0.2500\n",
      "Train Epoch: 1[16000/20000] \t Train loss: 0.6936 \t Train acc: 0.5000\n",
      "Train Epoch: 1[16800/20000] \t Train loss: 0.6932 \t Train acc: 0.5000\n",
      "Train Epoch: 1[17600/20000] \t Train loss: 0.6941 \t Train acc: 0.7500\n",
      "Train Epoch: 1[18400/20000] \t Train loss: 0.6948 \t Train acc: 1.0000\n",
      "Train Epoch: 1[19200/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "\n",
      "Epoch: 1 \t Train_loss: 0.6932 \t Train_acc: 0.4986 \t Valid_loss: 0.6932 \t Valid_acc: 0.5054\n",
      "Train Epoch: 2[0/20000] \t Train loss: 0.6926 \t Train acc: 0.0000\n",
      "Train Epoch: 2[800/20000] \t Train loss: 0.6932 \t Train acc: 0.5000\n",
      "Train Epoch: 2[1600/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 2[2400/20000] \t Train loss: 0.6932 \t Train acc: 0.5000\n",
      "Train Epoch: 2[3200/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 2[4000/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 2[4800/20000] \t Train loss: 0.6933 \t Train acc: 0.7500\n",
      "Train Epoch: 2[5600/20000] \t Train loss: 0.6931 \t Train acc: 0.2500\n",
      "Train Epoch: 2[6400/20000] \t Train loss: 0.6930 \t Train acc: 0.2500\n",
      "Train Epoch: 2[7200/20000] \t Train loss: 0.6930 \t Train acc: 0.2500\n",
      "Train Epoch: 2[8000/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 2[8800/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 2[9600/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 2[10400/20000] \t Train loss: 0.6932 \t Train acc: 0.7500\n",
      "Train Epoch: 2[11200/20000] \t Train loss: 0.6932 \t Train acc: 0.7500\n",
      "Train Epoch: 2[12000/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 2[12800/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 2[13600/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 2[14400/20000] \t Train loss: 0.6931 \t Train acc: 0.0000\n",
      "Train Epoch: 2[15200/20000] \t Train loss: 0.6933 \t Train acc: 0.7500\n",
      "Train Epoch: 2[16000/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 2[16800/20000] \t Train loss: 0.6911 \t Train acc: 0.2500\n",
      "Train Epoch: 2[17600/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 2[18400/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 2[19200/20000] \t Train loss: 0.6932 \t Train acc: 0.5000\n",
      "\n",
      "Epoch: 2 \t Train_loss: 0.6932 \t Train_acc: 0.4986 \t Valid_loss: 0.6932 \t Valid_acc: 0.5054\n",
      "Train Epoch: 3[0/20000] \t Train loss: 0.6962 \t Train acc: 0.7500\n",
      "Train Epoch: 3[800/20000] \t Train loss: 0.6879 \t Train acc: 0.2500\n",
      "Train Epoch: 3[1600/20000] \t Train loss: 0.6952 \t Train acc: 0.7500\n",
      "Train Epoch: 3[2400/20000] \t Train loss: 0.6891 \t Train acc: 0.0000\n",
      "Train Epoch: 3[3200/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 3[4000/20000] \t Train loss: 0.6947 \t Train acc: 0.7500\n",
      "Train Epoch: 3[4800/20000] \t Train loss: 0.6932 \t Train acc: 0.5000\n",
      "Train Epoch: 3[5600/20000] \t Train loss: 0.6932 \t Train acc: 0.5000\n",
      "Train Epoch: 3[6400/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 3[7200/20000] \t Train loss: 0.6922 \t Train acc: 0.2500\n",
      "Train Epoch: 3[8000/20000] \t Train loss: 0.6939 \t Train acc: 0.7500\n",
      "Train Epoch: 3[8800/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 3[9600/20000] \t Train loss: 0.6932 \t Train acc: 0.5000\n",
      "Train Epoch: 3[10400/20000] \t Train loss: 0.6943 \t Train acc: 1.0000\n",
      "Train Epoch: 3[11200/20000] \t Train loss: 0.6940 \t Train acc: 0.7500\n",
      "Train Epoch: 3[12000/20000] \t Train loss: 0.6934 \t Train acc: 0.7500\n",
      "Train Epoch: 3[12800/20000] \t Train loss: 0.6934 \t Train acc: 0.7500\n",
      "Train Epoch: 3[13600/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 3[14400/20000] \t Train loss: 0.6932 \t Train acc: 0.5000\n",
      "Train Epoch: 3[15200/20000] \t Train loss: 0.6932 \t Train acc: 0.5000\n",
      "Train Epoch: 3[16000/20000] \t Train loss: 0.6929 \t Train acc: 0.2500\n",
      "Train Epoch: 3[16800/20000] \t Train loss: 0.6933 \t Train acc: 0.7500\n",
      "Train Epoch: 3[17600/20000] \t Train loss: 0.6932 \t Train acc: 0.5000\n",
      "Train Epoch: 3[18400/20000] \t Train loss: 0.6921 \t Train acc: 0.2500\n",
      "Train Epoch: 3[19200/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "\n",
      "Epoch: 3 \t Train_loss: 0.6932 \t Train_acc: 0.4986 \t Valid_loss: 0.6932 \t Valid_acc: 0.5054\n",
      "Train Epoch: 4[0/20000] \t Train loss: 0.6940 \t Train acc: 0.7500\n",
      "Train Epoch: 4[800/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 4[1600/20000] \t Train loss: 0.6939 \t Train acc: 0.7500\n",
      "Train Epoch: 4[2400/20000] \t Train loss: 0.6935 \t Train acc: 0.7500\n",
      "Train Epoch: 4[3200/20000] \t Train loss: 0.6928 \t Train acc: 0.2500\n",
      "Train Epoch: 4[4000/20000] \t Train loss: 0.6932 \t Train acc: 0.5000\n",
      "Train Epoch: 4[4800/20000] \t Train loss: 0.6929 \t Train acc: 0.2500\n",
      "Train Epoch: 4[5600/20000] \t Train loss: 0.6935 \t Train acc: 0.7500\n",
      "Train Epoch: 4[6400/20000] \t Train loss: 0.6934 \t Train acc: 0.7500\n",
      "Train Epoch: 4[7200/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 4[8000/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 4[8800/20000] \t Train loss: 0.6930 \t Train acc: 0.2500\n",
      "Train Epoch: 4[9600/20000] \t Train loss: 0.6930 \t Train acc: 0.2500\n",
      "Train Epoch: 4[10400/20000] \t Train loss: 0.6932 \t Train acc: 0.5000\n",
      "Train Epoch: 4[11200/20000] \t Train loss: 0.6933 \t Train acc: 0.7500\n",
      "Train Epoch: 4[12000/20000] \t Train loss: 0.6930 \t Train acc: 0.0000\n",
      "Train Epoch: 4[12800/20000] \t Train loss: 0.6932 \t Train acc: 0.7500\n",
      "Train Epoch: 4[13600/20000] \t Train loss: 0.6932 \t Train acc: 0.5000\n",
      "Train Epoch: 4[14400/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 4[15200/20000] \t Train loss: 0.6931 \t Train acc: 0.2500\n",
      "Train Epoch: 4[16000/20000] \t Train loss: 0.6931 \t Train acc: 0.2500\n",
      "Train Epoch: 4[16800/20000] \t Train loss: 0.6925 \t Train acc: 0.2500\n",
      "Train Epoch: 4[17600/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 4[18400/20000] \t Train loss: 0.6929 \t Train acc: 0.2500\n",
      "Train Epoch: 4[19200/20000] \t Train loss: 0.6913 \t Train acc: 0.0000\n",
      "\n",
      "Epoch: 4 \t Train_loss: 0.6932 \t Train_acc: 0.4986 \t Valid_loss: 0.6932 \t Valid_acc: 0.5054\n",
      "Train Epoch: 5[0/20000] \t Train loss: 0.6918 \t Train acc: 0.0000\n",
      "Train Epoch: 5[800/20000] \t Train loss: 0.6919 \t Train acc: 0.0000\n",
      "Train Epoch: 5[1600/20000] \t Train loss: 0.6922 \t Train acc: 0.2500\n",
      "Train Epoch: 5[2400/20000] \t Train loss: 0.6916 \t Train acc: 0.0000\n",
      "Train Epoch: 5[3200/20000] \t Train loss: 0.6925 \t Train acc: 0.2500\n",
      "Train Epoch: 5[4000/20000] \t Train loss: 0.6925 \t Train acc: 0.5000\n",
      "Train Epoch: 5[4800/20000] \t Train loss: 0.6930 \t Train acc: 0.5000\n",
      "Train Epoch: 5[5600/20000] \t Train loss: 0.6933 \t Train acc: 0.5000\n",
      "Train Epoch: 5[6400/20000] \t Train loss: 0.6932 \t Train acc: 0.5000\n",
      "Train Epoch: 5[7200/20000] \t Train loss: 0.6958 \t Train acc: 0.7500\n",
      "Train Epoch: 5[8000/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 5[8800/20000] \t Train loss: 0.6923 \t Train acc: 0.2500\n",
      "Train Epoch: 5[9600/20000] \t Train loss: 0.6937 \t Train acc: 0.7500\n",
      "Train Epoch: 5[10400/20000] \t Train loss: 0.6937 \t Train acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5[11200/20000] \t Train loss: 0.6927 \t Train acc: 0.2500\n",
      "Train Epoch: 5[12000/20000] \t Train loss: 0.6928 \t Train acc: 0.2500\n",
      "Train Epoch: 5[12800/20000] \t Train loss: 0.6929 \t Train acc: 0.2500\n",
      "Train Epoch: 5[13600/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 5[14400/20000] \t Train loss: 0.6929 \t Train acc: 0.2500\n",
      "Train Epoch: 5[15200/20000] \t Train loss: 0.6933 \t Train acc: 0.7500\n",
      "Train Epoch: 5[16000/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 5[16800/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 5[17600/20000] \t Train loss: 0.6930 \t Train acc: 0.2500\n",
      "Train Epoch: 5[18400/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "Train Epoch: 5[19200/20000] \t Train loss: 0.6931 \t Train acc: 0.5000\n",
      "\n",
      "Epoch: 5 \t Train_loss: 0.6932 \t Train_acc: 0.4986 \t Valid_loss: 0.6932 \t Valid_acc: 0.5054\n"
     ]
    }
   ],
   "source": [
    "''' 8. Training '''\n",
    "N_EPOCH = 5\n",
    "model_name = \"BERT\"\n",
    "\n",
    "print('---------------------------------')\n",
    "print(f'Model name : {model_name}')\n",
    "print('---------------------------------')\n",
    "for epoch in range(1,N_EPOCH+1):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, loss, epoch, **model_config)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, optimizer, loss, epoch, **model_config)\n",
    "    print(\"\\nEpoch: {} \\t Train_loss: {:.4f} \\t Train_acc: {:.4f} \\t Valid_loss: {:.4f} \\t Valid_acc: {:.4f}\".format(\n",
    "                                    epoch, train_loss, train_acc, valid_loss, valid_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2b147873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU memory 부족할 때\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2_py38",
   "language": "python",
   "name": "tensorflow2_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
