{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de4130d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DistilBERT\n",
    "# 지식증류라는 것은 무엇일까??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f14d9cf",
   "metadata": {},
   "source": [
    "- 지식증류: 사전 학습된 대형 모델의 동작을 재현하기 위해 소형 모델을 학습시키는 모델 압축 기술이다.  \n",
    "\n",
    "\n",
    "- 또한 대형 모델(교사 네트워크)이 소프트맥스를 통해 분류할 때 argmax를 제외하고 그 다음으로 높은 값을 나타내는 단어들이 있을 것이다. 그 단어들을 가지고 소형 모델이 학습하도록 하는데, 우리는 이를 '암흑 지식'이라고 한다.  \n",
    "- 만약 성능이 너무 좋은 대형 모델이라면, argmax값이 너무 극단적(0.989)으로 나와서 나머지 값들로 학습하기가 힘들어지는데, 이를 해결하고 T(temperature)를 이용한다. 이는 소프트 맥수 함수에 들어가는 값에 x/T 를 해주는 것이다.   \n",
    "\n",
    "\n",
    "- T=1이면, 일반적인 소프트 맥스이고, 값이 더 커질수록 더 평활화가 이루어진다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7817fd77",
   "metadata": {},
   "source": [
    "# 암흑 지식 학습 방법\n",
    "\n",
    "#### 증류 손실\n",
    "1. 일단 대형 모델에서 나온 값을 소프트 타깃이라고 하고, 학생 네트워크에서 나온 값을 소프트 예측 이라고 한다.  \n",
    "2. 여기서 소프트 타깃과 소프트 예측 간의 에러를 계산해서 역전파를 한다.  \n",
    "+) 보통 T를 1보다 크게 한다.  (밑에 나오는 하드 예측, 하드 타깃과 연관이 된다.)  \n",
    "\n",
    "#### 학생손실\n",
    "1. T=1로 설정해서 학생 네트워크에서 값을 예측한다. 이를 하드 예측이라고 한다.  \n",
    "2. 레이블을 기반으로 하드 타깃을 가져온다.  \n",
    "3. 하드 타깃과 하드 예측 간의 에러를 계산한다.  \n",
    "\n",
    "\n",
    "#### 최종 손실 함수\n",
    "L = ax학생손실 + bx증류손실"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074d1e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2_py38",
   "language": "python",
   "name": "tensorflow2_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
