{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e85ff077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/hyunwoongko/transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8438d9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/nawnoes/pytorch-transformer/blob/main/model/transformer.py\n",
    "# 간단한 transformer만 구현해보도록 하겠다.\n",
    "# 2-th block 코드 읽고 해석용 (pytorch 기반)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed312639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kyujin Han\n",
    "# 2022-02-13\n",
    "# Title: Transformers 구현하기 (BERT 및 VIT 공부를 위한 Self-Study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e6dab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주의할점 <아닐 수도 있지만, 경험에 의해서>\n",
    "# 코드를 읽을 때 mask를 하나의 다른 vector(행렬)이라고 생각해야한다.\n",
    "# 즉, masking하기 위한 위치를 표현하는 행렬(벡터)라는 것이다.\n",
    "# 구현방식에 따라 차이가 있겠지만, 해당 코드는 그러한 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5727d5",
   "metadata": {},
   "source": [
    "# 'Attention Is All You Need' 논문 읽고 난 후 코드 읽기를 추천"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb58fe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 1. Import Module \"\"\"\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable # 옛날 pytorch에서는 필요했지만, 최신버전이라면 필요는 없다. 그러나 github 코드와 똑같이 구현해보겠다.\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from model.util import clones\n",
    "from transformers.activations import get_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e13c170a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.9775, -0.0458,  0.8604],\n",
      "         [-0.8800, -0.4634,  2.5793],\n",
      "         [-2.5340,  0.8712,  2.1390],\n",
      "         [-0.8467,  0.4708, -1.1788]],\n",
      "\n",
      "        [[-0.4873, -0.3576,  2.0115],\n",
      "         [ 0.1328,  1.0732, -0.1254],\n",
      "         [-0.5781,  0.3483,  1.1489],\n",
      "         [ 2.3782,  0.8393, -0.2120]]])\n",
      "tensor([[[-1.9775, -0.8800, -2.5340, -0.8467],\n",
      "         [-0.4873,  0.1328, -0.5781,  2.3782]],\n",
      "\n",
      "        [[-0.0458, -0.4634,  0.8712,  0.4708],\n",
      "         [-0.3576,  1.0732,  0.3483,  0.8393]],\n",
      "\n",
      "        [[ 0.8604,  2.5793,  2.1390, -1.1788],\n",
      "         [ 2.0115, -0.1254,  1.1489, -0.2120]]])\n"
     ]
    }
   ],
   "source": [
    "# Transpose 설명 (모르는 사람들을 위한 간단한 실습)\n",
    "a = torch.randn(2,3,4)\n",
    "print(torch.transpose(a,2,1)) # (2,4,3)\n",
    "print(a.transpose(0,1)) # (3,2,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85333025",
   "metadata": {},
   "source": [
    "self-Attention의 경우 Query Q, Key K, Value V를 입력으로 받아   \n",
    "MatMul(Q,K) -> Scale -> Masking(opt. Decoder) -> Softmax -> MatMul(result, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9b91bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 2-1. Self-Attention 구현 '''\n",
    "def self_attention(query, key, value, mask=None):\n",
    "    # input 4 dimensional vector\n",
    "    # [batch_size, head_num, length, d_tensor]\n",
    "    \n",
    "    key_transpose = torch.transpose(key,-2,-1) # (batch_Size, head_num, d_k, token_) \n",
    "    matmul_result = torch.matmul(query, key_transpose)\n",
    "    d_k = query.size()[-1]\n",
    "    attention_score = matmul_result/math.sqrt(d_k) # Scale\n",
    "    \n",
    "    # Masked Multi head Attention에 필요한 조건\n",
    "    if mask is not None:\n",
    "        attention_score = attention_score.masked_fill(mask==0, -1e20) # mask가 0인 곳은 {음의 무한대}로\n",
    "    \n",
    "    softmax_attention_score = F.softmax(attention_score, dim=-1) # score 행렬\n",
    "    result = torch.matmul(softmax_attention_score, value)\n",
    "    \n",
    "    return result, sofmax_attention_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8244e48",
   "metadata": {},
   "source": [
    "멀티헤드 어텐션  \n",
    "MultiHead(Q,K,V) = Concat(head_1,head_2,...head_n)W^O  \n",
    "head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)  \n",
    "W^Q는 모델의 dimension x d_k  \n",
    "W^K는 모델의 dimension x d_k  \n",
    "W^V는 모델의 dimension x d_v  \n",
    "W^O는 d_v * head 갯수 x 모델 dimension  \n",
    "논문에서는 헤더의 갯수를 8개 사용  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1a6f457",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 2-2. Multi-Head-Attention 구현 '''\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, head_num=8, d_model=512, dropout=0.1): # 모두 논문의 수치\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.head_num = head_num\n",
    "        self.d_model = d_model\n",
    "        self.d_k = self.d_v = d_model // head_num # 64\n",
    "        \n",
    "        # 논문을 바탕으로 코드 수정함\n",
    "        self.w_q = nn.Linear(d_model, d_k)\n",
    "        self.w_k = nn.Linear(d_model, d_k)\n",
    "        self.w_v = nn.Linear(d_model, d_v)\n",
    "        self.w_o = nn.Linear(self.head_num*d_model, d_model)\n",
    "        \n",
    "        self.self_attention = self_attention\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            # same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1) # 차원이 1인 dimension 추가 (mask 차원에)\n",
    "            \n",
    "        batche_num = query.size(0)\n",
    "        \n",
    "        query = self.w_q(query).view(batche_num, -1, self.head_num, self.d_k).transpose(1,2) # (batch, head_num, token_, d_k)\n",
    "        key = self.w_k(key).view(batche_num, -1, self.head_num, self.d_k).transpose(1,2)\n",
    "        value = self.w_v(value).view(batche_num, -1, self.head_num, self.d_k).transpose(1,2)\n",
    "        \n",
    "        attention_result, attention_score = self.self_attention(query, key, value, mask)\n",
    "        \n",
    "        # 원래의 모양으로 다시 변형해준다.\n",
    "        # torch.continuos는 다음행과 열로 이동하기 위한 stride가 변형되어\n",
    "        # 메모리 연속적으로 바꿔야 한다!\n",
    "        # 참고 문서: https://discuss.pytorch.org/t/contigious-vs-non-contigious-tensor/30107/2\n",
    "        attention_result = attention_result.transpose(1,2).contiguous().view(batche_num, -1, self.head_num*self.d_k)\n",
    "        \n",
    "        return self.w_o(attention_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a623e79",
   "metadata": {},
   "source": [
    "Position-wise Feed-Forward Networks  \n",
    "FFN(x) = max(0,xW_1 + b_1)W_2+b2  \n",
    "입력과 출력은 모두 d_model의 dimension을 가지고  \n",
    "내부의 레이어는 d_model * 4의 dimension을 가진다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9badc2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 3. Feed Forward Network '''\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,d_model, dropout = 0.1):\n",
    "        super(FeedForward,self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_model*4) # (512, 2048)\n",
    "        self.w_2 = nn.Linear(d_model*4, d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x)))) # Use relu activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a0b70a",
   "metadata": {},
   "source": [
    "Layer Normalization  \n",
    ": layer의 hidden unit들에 대해서 mean과 variance를 구한다.   \n",
    "nn.Parameter는 모듈 파라미터로 여겨지는 텐서  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7babc0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 4. Layer Normalization and Residual connection '''\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm,self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features)) # tensor 자료형 유지를 위해 있는 것으로 생각. (왜 필요한지 모르겠음)\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim =True) # 평균\n",
    "        std = x.std(-1, keepdim=True)    # 표준편차\n",
    "\n",
    "        return self.a_2 * (x-mean)/ (std + self.eps) + self.b_2 # Normalization\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, size, dropout):\n",
    "        super(RedisualConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout((sublayer(self.norm(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48978da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 5. Encoder '''\n",
    "# MultiHeadAttention + FeedForward\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, head_num, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model=d_model, head_num=head_num)\n",
    "        self.residual_1 = ResidualConnection(d_model, dropout=dropout)\n",
    "        \n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        self.residual_2 = ResidualConnection(d_model, dropout=dropout)\n",
    "        \n",
    "    def forward(self, input, mask):\n",
    "        x = self.residual_1(input, lambda x: self.multi_head_attention(x,x,x,mask)) # 논문에서 query, key, value의 weight을 공유.\n",
    "        x = self.residual_2(x, lambda x: self.feed_forward(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f42007",
   "metadata": {},
   "source": [
    "Decoder 블록은 FeedForward 레이어와 MultiHead 어텐션, Masked Multihead 어텐션 레이어를 가진다.  \n",
    "MaskedMultiHeadAttention -> MultiHeadAttention(encoder-decoder attention) -> FeedForward  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d38b71db",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 6. Decoder '''\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, head_num, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.masked_multi_head_attention = MultiHeadAttention(d_model = d_model, head_num = head_num)\n",
    "        self.residual_1 = ResidualConnection(d_model, dropout = dropout)\n",
    "        \n",
    "        self.encoder_decoder_attention = MultiHeadAttention(d_model = d_model, head_num = head_num)\n",
    "        self.residual_2 = ResidualConnection(d_model, dropout = dropout)\n",
    "        \n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        self.residual_3 = ResidualConnection(d_model, dropout = dropout)\n",
    "        \n",
    "    def forward(self, target, encoder_output, target_mask, encoder_mask):\n",
    "        \"\"\"\n",
    "        Residual_2 부분에 대한 추가 설명\n",
    "        위와 다르게 이 부분은 Self attention이라고 부르지 않는다.\n",
    "        Encoder-Decoder-attention 이라고 하는데, Query 값은 이전 SubLayer의 Masked_Multi-Head-Attention의 값에서,\n",
    "        그리고 key와 Value 값은 Encoder의 Output값에서 만든다.\n",
    "        \"\"\"\n",
    "        x = self.residual_1(target, lambda x: self.masked_multi_head_attention(x,x,x,target_mask))\n",
    "        x = self.residual_2(x, lambda x: self.encoder_decoder_attention(x, encoder_output, encoder_output, encoder_mask))\n",
    "        x = self.residual_3(x, self.feed_forward)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f88e18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 7. Embedding Vector '''\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_num, d_model):\n",
    "        super(Embeddings,self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_num,d_model)\n",
    "        self.d_model = d_model\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        1) 임베딩 값에 math.sqrt(self.d_model)을 곱해주는 이유는 무엇인지 찾아볼것 ==> 논문 3.4 근거\n",
    "        2) nn.Embedding에 다시 한번 찾아볼것\n",
    "        \"\"\"\n",
    "        return self.emb(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35d268d",
   "metadata": {},
   "source": [
    "Positional Encoding  \n",
    "트랜스포머는 RNN이나 CNN을 사용하지 않기 때문에 입력에 순서 값을 반영해줘야 한다.  \n",
    "예) 나는 어제의 오늘  \n",
    "PE (pos,2i) = sin(pos/10000^(2i/d_model))  \n",
    "PE (pos,2i+1) = cos(pos/10000^(2i/d_model))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8da4cb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 8. Positional Encoding '''\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_seq_len, d_model, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        \n",
    "        # 논문의 sins and cosine functions 참고\n",
    "        position = torch.arange(0, max_seq_len).unsqeeze(1) # 1D => 2D\n",
    "        base = torch.ones(d_model//2).fill_(10000)\n",
    "        pow_term = torch.arange(0, d_model, 2)/torch.tensor(d_model, dtype=torch.float32)\n",
    "        div_term = torch.pow(base, pow_term)\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position/div_term)\n",
    "        pe[:, 1::2] = torch.cos(position/div_term)\n",
    "        \n",
    "        # pe를 학습되지 않는 변수로 등록\n",
    "        self.register_buffer('position_encoding',pe) # self.positional_encoding 밑의 변수\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x+Variable(self.positional_encoding[:,:x.size(1)], requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(max_seq_len, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        t = torch.arange(x.shape[1], device=x.device) # 순서 저장 (embedding vector 길이)\n",
    "        return self.embedding(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "168ea7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 9. Linear 부분 (Decoder 이후)'''\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, d_model, vocab_num):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj_1 = nn.Linear(d_model, d_model*4)\n",
    "        self.proj_2 = nn.Linear(d_model*4, vocab_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj_1(x)\n",
    "        x = self.proj_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "627b30d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://incredible.ai/nlp/2020/02/29/Transformer/\n",
    "# Part 2.4 \n",
    "\n",
    "# Masking에 대한 자세한 설명과, 코드에서 unsqueeze()과 어떻게 masking 역할을 할 수 있는지에 대한 설명.\n",
    "\n",
    "## 추가자료\n",
    "## +) # https://paul-hyun.github.io/transformer-01/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d7e9b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 10. Transformer 구현 '''\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_num, d_model, max_seq_len, head_num, dropout, N):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embedding = Embeddings(vocab_num, d_model) # input Embeddings\n",
    "        self.positional_encoding = PositionalEncoding(max_seq_len, d_model) # positional embedding\n",
    "\n",
    "        self.encoders = clones(Encoder(d_model=d_model, head_num=head_num, dropout=dropout),N) # 논문 기준, N=6\n",
    "        self.decoders = clones(Decoder(d_model=d_model, head_num=head_num, dropout=dropout),N) # N=6\n",
    "\n",
    "        self.generator = Generator(d_model, vocab_num)\n",
    "    \n",
    "    # BPE(byte-pair-encoding)을 통해 tokenize를 진행\n",
    "    # \n",
    "    def forward(self, input, target, input_mask, target_mask, labels=None):\n",
    "        \"\"\"\n",
    "        1) input값을 embedding vector로 만들어 준 다음에 positional encoding을 한다.\n",
    "        2) 그리고 encoder에 넣는다.\n",
    "        \"\"\"\n",
    "        x = self.positional_encoding(self.embedding(input))\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x, input_mask) # input_mask 같은 경우에는 이제 pad의 위치를 담고 있는 것이 아닐까 생각해본다.\n",
    "        \n",
    "        \"\"\"\n",
    "        3) 타깃문장을 넣어서 embedding vector를 생성한 후 positional encoding을 한다.\n",
    "        4) decoder를 실행한다.\n",
    "        \"\"\"\n",
    "        target = self.positional_encoding(self.embedding(target))\n",
    "        for decoder in self.decoders:\n",
    "            target = decoder(target, x, target_mask, input_mask) # target_mask는 mask를 할려는 위치를 check한 것이 아닐까 생각해본다.\n",
    "        \n",
    "        '''\n",
    "        5) decoder에서 나온 값을 linear 결합한다.\n",
    "        # 원래 decoder 마지막에는 linear에서 나온 값을 바탕으로 softmax가 있어야 한다.\n",
    "        Ex code) \n",
    "            prediction = lm_logits.view(-1, lm_logits.size(-1))\n",
    "            result = F.softmax(prediction, dim=1)\n",
    "            pred_label = torch.argmax(result, dim=1)\n",
    "        (스스로 생각해서 작성한거라 틀릴 수도 있음. 그러나 이런 형태로 만들어지는건 틀림없음.)\n",
    "        '''\n",
    "        lm_logits = self.generator(target)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens<n predict n\n",
    "            shift_logits = lm_logits[...,:-1,:].contiguous()\n",
    "            shift_labels = labels[...,1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntroptLoss(ignore_index=0)\n",
    "            loss = loss_fct(shift_logits.view(-1,shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            \n",
    "        return lm_logits, loss\n",
    "    \n",
    "    def encode(self, input, input_mask):\n",
    "        x = self.positional_encoding(self.embedding(input))\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x, input_mask)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def decode(self, encode_output, encoder_mask, target, target_mask):\n",
    "        x = self.positional_encoding(self.embedding(input))\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x, input_mask)\n",
    "        \n",
    "        target = self.positional_encoding(self.embedding(target))\n",
    "        for decoder in self.decoders:\n",
    "            target = decoder(target, x, target_mask, input_mask)\n",
    "        \n",
    "        lm_logits = self.generator(target)\n",
    "        \n",
    "        return lm_logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2_py38",
   "language": "python",
   "name": "tensorflow2_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
